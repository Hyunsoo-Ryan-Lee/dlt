{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt, requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50a2d1c4-d788-4498-a6f7-dd75d4db588f</td>\n",
       "      <td>Stephanie Love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>438005c2-dd1d-48aa-8bfd-7fb06851b5f8</td>\n",
       "      <td>Kristi Keller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5261268c-aa94-438a-921a-05efc0d414ac</td>\n",
       "      <td>Allison Oliver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f8486fce-bc07-4a4f-a6e9-ed6a06ba996c</td>\n",
       "      <td>Nancy Austin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>341ed9b2-1760-4720-a1b1-42681d273c63</td>\n",
       "      <td>Christy Rios</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id            name\n",
       "0  50a2d1c4-d788-4498-a6f7-dd75d4db588f  Stephanie Love\n",
       "1  438005c2-dd1d-48aa-8bfd-7fb06851b5f8   Kristi Keller\n",
       "2  5261268c-aa94-438a-921a-05efc0d414ac  Allison Oliver\n",
       "3  f8486fce-bc07-4a4f-a6e9-ed6a06ba996c    Nancy Austin\n",
       "4  341ed9b2-1760-4720-a1b1-42681d273c63    Christy Rios"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URL = \"https://jaffle-shop.scalevector.ai/api/v1/customers\"\n",
    "\n",
    "req = requests.get(URL)\n",
    "\n",
    "df = pd.json_normalize(req.json())\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>relative_path</th>\n",
       "      <th>file_url</th>\n",
       "      <th>mime_type</th>\n",
       "      <th>encoding</th>\n",
       "      <th>modification_date</th>\n",
       "      <th>size_in_bytes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>emp.csv</td>\n",
       "      <td>emp.csv</td>\n",
       "      <td>s3://lakeformation-demo-hyunsoo/dlt_dataset/em...</td>\n",
       "      <td>text/csv</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-07-09 06:17:31+00:00</td>\n",
       "      <td>1263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  file_name relative_path                                           file_url  \\\n",
       "0   emp.csv       emp.csv  s3://lakeformation-demo-hyunsoo/dlt_dataset/em...   \n",
       "\n",
       "  mime_type encoding         modification_date  size_in_bytes  \n",
       "0  text/csv     None 2025-07-09 06:17:31+00:00           1263  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. dlt를 사용하여 S3의 CSV 파일을 데이터 소스로 읽어오는 코드 예시입니다.\n",
    "\n",
    "import dlt\n",
    "import pandas as pd\n",
    "from dlt.sources.filesystem import filesystem\n",
    "\n",
    "# dlt의 s3 파일 커넥터를 사용하여 S3의 CSV 파일을 읽어옵니다.\n",
    "# dlt.sources.filesystem.filesystem_source를 사용합니다.\n",
    "\n",
    "from dlt.sources.filesystem import filesystem\n",
    "\n",
    "filesystem_source = filesystem(\n",
    "  bucket_url=\"s3://lakeformation-demo-hyunsoo/dlt_dataset/\",\n",
    "  file_glob=\"*.csv\"\n",
    ")\n",
    "\n",
    "# 데이터를 DataFrame으로 변환\n",
    "emp_df = pd.DataFrame(list(filesystem_source))\n",
    "\n",
    "# 데이터 확인\n",
    "emp_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-09 22:06:32,457|[WARNING]|11574|8685697728|dlt|validate.py|verify_normalized_table:57|In schema `emp_data_pg_pipeline3`: The following columns in table 'employees3' did not receive any data during this load and therefore could not have their types inferred:\n",
      "  - encoding\n",
      "\n",
      "Unless type hints are provided, these columns will not be materialized in the destination.\n",
      "One way to provide type hints is to use the 'columns' argument in the '@dlt.resource' decorator.  For example:\n",
      "\n",
      "@dlt.resource(columns={'encoding': {'data_type': 'text'}})\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 CSV 데이터를 PostgreSQL에 저장한 결과: Pipeline emp_data_pg_pipeline3 load step completed in 0.05 seconds\n",
      "1 load package(s) were loaded to destination postgres and into dataset emp_data_pg\n",
      "The postgres destination used postgresql://codeit:***@localhost:5432/postgres location to store data\n",
      "Load package 1752066392.3051882 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "# S3에서 읽은 emp_df(DataFrame)를 PostgreSQL에 저장하는 dlt 파이프라인 생성\n",
    "emp_pg_pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"emp_data_pg_pipeline3\",\n",
    "    destination=\"postgres\",\n",
    "    dataset_name=\"emp_data_pg\"\n",
    ")\n",
    "\n",
    "# emp_df를 \"employees\" 테이블로 적재\n",
    "emp_pg_load_info = emp_pg_pipeline.run(\n",
    "    data=filesystem_source,\n",
    "    table_name=\"employees3\"\n",
    ")\n",
    "\n",
    "print(\"S3 CSV 데이터를 PostgreSQL에 저장한 결과:\", emp_pg_load_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"customer_data_pipeline\",\n",
    "    destination=\"filesystem\",\n",
    "    dataset_name=\"customer_data\"\n",
    ")\n",
    "\n",
    "load_info = pipeline.run(\n",
    "    data=df, \n",
    "    table_name=\"customers\"\n",
    "    )\n",
    "\n",
    "print(\"파이프라인 실행 결과:\", load_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet('./api_data_dir/customer_data/customers/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "\n",
    "# PostgreSQL로 데이터를 저장하는 dlt 파이프라인 생성\n",
    "# 동일 코드라도 데이터가 다르면 pipeline_name을 다르게 설정해야 함\n",
    "pg_pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"customer_data_pg_pipeline2\",\n",
    "    destination=\"postgres\",\n",
    "    dataset_name=\"customer_data_pg\"\n",
    ")\n",
    "\n",
    "# 데이터 적재 실행\n",
    "pg_load_info = pg_pipeline.run(\n",
    "    data=df,\n",
    "    table_name=\"stores\"\n",
    ")\n",
    "\n",
    "print(\"PostgreSQL 파이프라인 실행 결과:\", pg_load_info)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-09 21:48:57,939|[WARNING]|10856|8685697728|dlt|pipeline.py|_set_destinations:1438|The destination athena requires the filesystem staging destination to be set, but it was not provided. Setting it to 'filesystem'.\n",
      "2025-07-09 21:49:00,729|[WARNING]|10856|8685697728|dlt|pipeline.py|run:719|The pipeline `run` method will now load the pending load packages. The data you passed to the run function will not be loaded. In order to do that you must run the pipeline again\n"
     ]
    },
    {
     "ename": "PipelineStepFailed",
     "evalue": "Pipeline execution failed at `step=load` when processing package with `load_id=1752064658.196621` with exception:\n\n<class 'dlt.destinations.exceptions.DatabaseTerminalException'>\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:Insufficient Lake Formation permission(s) on s3://personal-golight-image-bucket/dataset/customer_data_athena/customers (Service: AmazonDataCatalog; Status Code: 400; Error Code: AccessDeniedException; Request ID: 0345ae86-2459-4af3-8778-486b7e60d51e; Proxy: null))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Personal_Studies/personal_venv/.venv/lib/python3.12/site-packages/dlt/destinations/sql_client.py:435\u001b[39m, in \u001b[36mraise_database_error.<locals>._wrap_gen\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    434\u001b[39m     \u001b[38;5;28mself\u001b[39m._ensure_native_conn()\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01myield from\u001b[39;00m f(\u001b[38;5;28mself\u001b[39m, *args, **kwargs))\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Personal_Studies/personal_venv/.venv/lib/python3.12/site-packages/dlt/destinations/impl/athena/sql_client.py:221\u001b[39m, in \u001b[36mAthenaSQLClient.execute_query\u001b[39m\u001b[34m(self, query, *args, **kwargs)\u001b[39m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m     \u001b[43mcursor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_line\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[38;5;66;03m# catch key error only here, this will show up if we have a missing parameter\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Personal_Studies/personal_venv/.venv/lib/python3.12/site-packages/pyathena/cursor.py:110\u001b[39m, in \u001b[36mCursor.execute\u001b[39m\u001b[34m(self, operation, parameters, work_group, s3_staging_dir, cache_size, cache_expiration_time, result_reuse_enable, result_reuse_minutes, paramstyle, **kwargs)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OperationalError(query_execution.state_change_reason)\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[31mOperationalError\u001b[39m: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:Insufficient Lake Formation permission(s) on s3://personal-golight-image-bucket/dataset/customer_data_athena/customers (Service: AmazonDataCatalog; Status Code: 400; Error Code: AccessDeniedException; Request ID: 0345ae86-2459-4af3-8778-486b7e60d51e; Proxy: null))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mDatabaseTerminalException\u001b[39m                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Personal_Studies/personal_venv/.venv/lib/python3.12/site-packages/dlt/pipeline/pipeline.py:598\u001b[39m, in \u001b[36mPipeline.load\u001b[39m\u001b[34m(self, destination, dataset_name, credentials, workers, raise_on_failed_jobs)\u001b[39m\n\u001b[32m    597\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m signals.delayed_signals():\n\u001b[32m--> \u001b[39m\u001b[32m598\u001b[39m     \u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_step\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    599\u001b[39m info: LoadInfo = \u001b[38;5;28mself\u001b[39m._get_step_info(load_step)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Personal_Studies/personal_venv/.venv/lib/python3.12/site-packages/dlt/common/runners/pool_runner.py:203\u001b[39m, in \u001b[36mrun_pool\u001b[39m\u001b[34m(config, run_f)\u001b[39m\n\u001b[32m    202\u001b[39m logger.debug(\u001b[33m\"\u001b[39m\u001b[33mRunning pool\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[43m_run_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    204\u001b[39m     \u001b[38;5;66;03m# for next run\u001b[39;00m\n\u001b[32m    205\u001b[39m     signals.raise_if_signalled()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Personal_Studies/personal_venv/.venv/lib/python3.12/site-packages/dlt/common/runners/pool_runner.py:196\u001b[39m, in \u001b[36mrun_pool.<locals>._run_func\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(run_f, Runnable):\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m     run_metrics = \u001b[43mrun_f\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTExecutor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Personal_Studies/personal_venv/.venv/lib/python3.12/site-packages/dlt/load/load.py:638\u001b[39m, in \u001b[36mLoad.run\u001b[39m\u001b[34m(self, pool)\u001b[39m\n\u001b[32m    637\u001b[39m             \u001b[38;5;28mself\u001b[39m._step_info_start_load_id(load_id)\n\u001b[32m--> \u001b[39m\u001b[32m638\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_single_package\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    640\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m TRunMetrics(\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.load_storage.list_normalized_packages()))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Personal_Studies/personal_venv/.venv/lib/python3.12/site-packages/dlt/load/load.py:527\u001b[39m, in \u001b[36mLoad.load_single_package\u001b[39m\u001b[34m(self, load_id, schema)\u001b[39m\n\u001b[32m    525\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (expected_update := \u001b[38;5;28mself\u001b[39m.load_storage.begin_schema_update(load_id)) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    526\u001b[39m     \u001b[38;5;66;03m# init job client\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m527\u001b[39m     applied_update = \u001b[43minit_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjob_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnew_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_update\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjob_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshould_truncate_table_before_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[43m            \u001b[49m\u001b[43mjob_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshould_load_data_to_staging_dataset\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mjob_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mWithStagingDataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    536\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m    537\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    538\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdrop_tables\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropped_tables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    539\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncate_tables\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncated_tables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    542\u001b[39m     \u001b[38;5;66;03m# init staging client\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Personal_Studies/personal_venv/.venv/lib/python3.12/site-packages/dlt/load/utils.py:117\u001b[39m, in \u001b[36minit_client\u001b[39m\u001b[34m(job_client, schema, new_jobs, expected_update, truncate_filter, load_staging_filter, drop_tables, truncate_tables)\u001b[39m\n\u001b[32m    116\u001b[39m job_client.verify_schema(only_tables=tables_with_jobs | dlt_tables, new_jobs=new_jobs)\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m applied_update = \u001b[43m_init_dataset_and_update_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjob_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexpected_update\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtables_with_jobs\u001b[49m\u001b[43m \u001b[49m\u001b[43m|\u001b[49m\u001b[43m \u001b[49m\u001b[43mdlt_tables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtruncate_table_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrop_tables\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdrop_table_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# update the staging dataset if client supports this\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Personal_Studies/personal_venv/.venv/lib/python3.12/site-packages/dlt/load/utils.py:181\u001b[39m, in \u001b[36m_init_dataset_and_update_schema\u001b[39m\u001b[34m(job_client, expected_update, update_tables, truncate_tables, staging_info, drop_tables)\u001b[39m\n\u001b[32m    177\u001b[39m logger.info(\n\u001b[32m    178\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mClient for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjob_client.config.destination_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m will update schema to package schema\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    179\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstaging_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    180\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m applied_update = \u001b[43mjob_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate_stored_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m    \u001b[49m\u001b[43monly_tables\u001b[49m\u001b[43m=\u001b[49m\u001b[43mupdate_tables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_update\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_update\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m truncate_tables:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Personal_Studies/personal_venv/.venv/lib/python3.12/site-packages/dlt/destinations/impl/athena/athena.py:338\u001b[39m, in \u001b[36mAthenaClient.update_stored_schema\u001b[39m\u001b[34m(self, only_tables, expected_update)\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupdate_stored_schema\u001b[39m(\n\u001b[32m    334\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    335\u001b[39m     only_tables: Iterable[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    336\u001b[39m     expected_update: TSchemaTables = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    337\u001b[39m ) -> Optional[TSchemaTables]:\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m     applied_update = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate_stored_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43monly_tables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_update\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_update\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m     \u001b[38;5;66;03m# here we could apply tags only if any migration happened, right now we do it on each run\u001b[39;00m\n\u001b[32m    340\u001b[39m     \u001b[38;5;66;03m# NOTE: tags are applied before any data is loaded\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Personal_Studies/personal_venv/.venv/lib/python3.12/site-packages/dlt/destinations/job_client_impl.py:310\u001b[39m, in \u001b[36mSqlJobClientBase.update_stored_schema\u001b[39m\u001b[34m(self, only_tables, expected_update)\u001b[39m\n\u001b[32m    309\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.maybe_ddl_transaction():\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m         applied_update = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_schema_update_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[43monly_tables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Personal_Studies/personal_venv/.venv/lib/python3.12/site-packages/dlt/destinations/job_client_impl.py:633\u001b[39m, in \u001b[36mSqlJobClientBase._execute_schema_update_sql\u001b[39m\u001b[34m(self, only_tables)\u001b[39m\n\u001b[32m    630\u001b[39m \u001b[38;5;66;03m# Stay within max query size when doing DDL.\u001b[39;00m\n\u001b[32m    631\u001b[39m \u001b[38;5;66;03m# Some DB backends use bytes not characters, so decrease the limit by half,\u001b[39;00m\n\u001b[32m    632\u001b[39m \u001b[38;5;66;03m# assuming most of the characters in DDL encoded into single bytes.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m633\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msql_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_many\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql_scripts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[38;5;28mself\u001b[39m._update_schema_in_storage(\u001b[38;5;28mself\u001b[39m.schema)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Personal_Studies/personal_venv/.venv/lib/python3.12/site-packages/dlt/destinations/sql_client.py:190\u001b[39m, in \u001b[36mSqlClientBase.execute_many\u001b[39m\u001b[34m(self, statements, *args, **kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m sql_fragment \u001b[38;5;129;01min\u001b[39;00m concat_strings_with_limit(\n\u001b[32m    188\u001b[39m         \u001b[38;5;28mlist\u001b[39m(statements), \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.capabilities.max_query_length // \u001b[32m2\u001b[39m\n\u001b[32m    189\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m         ret.append(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql_fragment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Personal_Studies/personal_venv/.venv/lib/python3.12/site-packages/dlt/destinations/impl/athena/sql_client.py:182\u001b[39m, in \u001b[36mAthenaSQLClient.execute_sql\u001b[39m\u001b[34m(self, sql, *args, **kwargs)\u001b[39m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute_sql\u001b[39m(\n\u001b[32m    180\u001b[39m     \u001b[38;5;28mself\u001b[39m, sql: AnyStr, *args: Any, **kwargs: Any\n\u001b[32m    181\u001b[39m ) -> Optional[Sequence[Sequence[Any]]]:\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcurr\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcurr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:137\u001b[39m, in \u001b[36m_GeneratorContextManager.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Personal_Studies/personal_venv/.venv/lib/python3.12/site-packages/dlt/destinations/sql_client.py:437\u001b[39m, in \u001b[36mraise_database_error.<locals>._wrap_gen\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_database_exception(ex)\n",
      "\u001b[31mDatabaseTerminalException\u001b[39m: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:Insufficient Lake Formation permission(s) on s3://personal-golight-image-bucket/dataset/customer_data_athena/customers (Service: AmazonDataCatalog; Status Code: 400; Error Code: AccessDeniedException; Request ID: 0345ae86-2459-4af3-8778-486b7e60d51e; Proxy: null))",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mPipelineStepFailed\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      4\u001b[39m athena_pipeline = dlt.pipeline(\n\u001b[32m      5\u001b[39m     pipeline_name=\u001b[33m\"\u001b[39m\u001b[33mcustomer_data_athena_pipeline\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     destination=\u001b[33m\"\u001b[39m\u001b[33mathena\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m     dataset_name=\u001b[33m\"\u001b[39m\u001b[33mcustomer_data_athena\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m )\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# 데이터 적재 실행\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m athena_load_info = \u001b[43mathena_pipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcustomers\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAthena 파이프라인 실행 결과:\u001b[39m\u001b[33m\"\u001b[39m, athena_load_info)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Personal_Studies/personal_venv/.venv/lib/python3.12/site-packages/dlt/pipeline/pipeline.py:228\u001b[39m, in \u001b[36mwith_runtime_trace.<locals>.decorator.<locals>._wrap\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trace:\n\u001b[32m    226\u001b[39m         trace_step = start_trace_step(trace, cast(TPipelineStep, f.\u001b[34m__name__\u001b[39m), \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m     step_info = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m step_info\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Personal_Studies/personal_venv/.venv/lib/python3.12/site-packages/dlt/pipeline/pipeline.py:277\u001b[39m, in \u001b[36mwith_config_section.<locals>.decorator.<locals>._wrap\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrap\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mPipeline\u001b[39m\u001b[33m\"\u001b[39m, *args: Any, **kwargs: Any) -> Any:\n\u001b[32m    271\u001b[39m     \u001b[38;5;66;03m# add section context to the container to be used by all configuration without explicit sections resolution\u001b[39;00m\n\u001b[32m    272\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inject_section(\n\u001b[32m    273\u001b[39m         ConfigSectionContext(\n\u001b[32m    274\u001b[39m             pipeline_name=\u001b[38;5;28mself\u001b[39m.pipeline_name, sections=sections, merge_style=merge_func\n\u001b[32m    275\u001b[39m         )\n\u001b[32m    276\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Personal_Studies/personal_venv/.venv/lib/python3.12/site-packages/dlt/pipeline/pipeline.py:724\u001b[39m, in \u001b[36mPipeline.run\u001b[39m\u001b[34m(self, data, destination, staging, dataset_name, credentials, table_name, write_disposition, columns, primary_key, schema, loader_file_format, table_format, schema_contract, refresh)\u001b[39m\n\u001b[32m    718\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    719\u001b[39m         logger.warn(\n\u001b[32m    720\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe pipeline `run` method will now load the pending load packages. The data\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    721\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m you passed to the run function will not be loaded. In order to do that you\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    722\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m must run the pipeline again\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    723\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m724\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    726\u001b[39m \u001b[38;5;66;03m# extract from the source\u001b[39;00m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Personal_Studies/personal_venv/.venv/lib/python3.12/site-packages/dlt/pipeline/pipeline.py:228\u001b[39m, in \u001b[36mwith_runtime_trace.<locals>.decorator.<locals>._wrap\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trace:\n\u001b[32m    226\u001b[39m         trace_step = start_trace_step(trace, cast(TPipelineStep, f.\u001b[34m__name__\u001b[39m), \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m     step_info = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m step_info\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Personal_Studies/personal_venv/.venv/lib/python3.12/site-packages/dlt/pipeline/pipeline.py:168\u001b[39m, in \u001b[36mwith_state_sync.<locals>.decorator.<locals>._wrap\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    166\u001b[39m should_extract_state = may_extract_state \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.restore_from_destination\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.managed_state(extract_state=should_extract_state):\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Personal_Studies/personal_venv/.venv/lib/python3.12/site-packages/dlt/pipeline/pipeline.py:277\u001b[39m, in \u001b[36mwith_config_section.<locals>.decorator.<locals>._wrap\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrap\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mPipeline\u001b[39m\u001b[33m\"\u001b[39m, *args: Any, **kwargs: Any) -> Any:\n\u001b[32m    271\u001b[39m     \u001b[38;5;66;03m# add section context to the container to be used by all configuration without explicit sections resolution\u001b[39;00m\n\u001b[32m    272\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inject_section(\n\u001b[32m    273\u001b[39m         ConfigSectionContext(\n\u001b[32m    274\u001b[39m             pipeline_name=\u001b[38;5;28mself\u001b[39m.pipeline_name, sections=sections, merge_style=merge_func\n\u001b[32m    275\u001b[39m         )\n\u001b[32m    276\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Personal_Studies/personal_venv/.venv/lib/python3.12/site-packages/dlt/pipeline/pipeline.py:604\u001b[39m, in \u001b[36mPipeline.load\u001b[39m\u001b[34m(self, destination, dataset_name, credentials, workers, raise_on_failed_jobs)\u001b[39m\n\u001b[32m    602\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m l_ex:\n\u001b[32m    603\u001b[39m     step_info = \u001b[38;5;28mself\u001b[39m._get_step_info(load_step)\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PipelineStepFailed(\n\u001b[32m    605\u001b[39m         \u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mload\u001b[39m\u001b[33m\"\u001b[39m, load_step.current_load_id, l_ex, step_info\n\u001b[32m    606\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01ml_ex\u001b[39;00m\n",
      "\u001b[31mPipelineStepFailed\u001b[39m: Pipeline execution failed at `step=load` when processing package with `load_id=1752064658.196621` with exception:\n\n<class 'dlt.destinations.exceptions.DatabaseTerminalException'>\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:Insufficient Lake Formation permission(s) on s3://personal-golight-image-bucket/dataset/customer_data_athena/customers (Service: AmazonDataCatalog; Status Code: 400; Error Code: AccessDeniedException; Request ID: 0345ae86-2459-4af3-8778-486b7e60d51e; Proxy: null))"
     ]
    }
   ],
   "source": [
    "import dlt\n",
    "\n",
    "# AWS Athena로 데이터를 저장하는 dlt 파이프라인 생성\n",
    "athena_pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"customer_data_athena_pipeline\",\n",
    "    destination=\"athena\",\n",
    "    dataset_name=\"customer_data_athena\"\n",
    ")\n",
    "\n",
    "# 데이터 적재 실행\n",
    "athena_load_info = athena_pipeline.run(\n",
    "    data=df,\n",
    "    table_name=\"customers\"\n",
    ")\n",
    "\n",
    "print(\"Athena 파이프라인 실행 결과:\", athena_load_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Athena에서 S3 버킷에 접근 권한이 없어서 발생하는 에러입니다.\n",
    "# Lake Formation 또는 S3 버킷의 권한 설정을 확인해야 합니다.\n",
    "# 아래는 문제 해결을 위한 안내 코드와 설명입니다.\n",
    "\n",
    "print(\"\"\"\n",
    "[문제 원인]\n",
    "- Athena가 S3 버킷(s3://personal-golight-image-bucket/dataset/customer_data_athena/_dlt_pipeline_state)에 접근할 권한이 없습니다.\n",
    "- Lake Formation 또는 S3 버킷의 권한 정책에서 Athena(및 Glue, Data Catalog) 역할/사용자에게 충분한 권한이 부여되어야 합니다.\n",
    "\n",
    "[해결 방법]\n",
    "1. AWS 콘솔에서 S3 버킷(s3://personal-golight-image-bucket)에 대해 다음 권한을 부여하세요.\n",
    "   - s3:GetObject, s3:PutObject, s3:DeleteObject, s3:ListBucket 등\n",
    "   - 권한을 부여할 주체: Athena, Glue, Data Catalog에서 사용하는 IAM Role 또는 User\n",
    "\n",
    "2. Lake Formation을 사용하는 경우:\n",
    "   - Lake Formation 콘솔에서 해당 S3 경로에 대해 Athena, Glue, Data Catalog에 '데이터 위치 권한'을 부여하세요.\n",
    "   - '데이터 위치' 등록 및 '데이터 위치 권한' 부여 필요\n",
    "\n",
    "3. 권한이 정상적으로 부여된 후 파이프라인을 다시 실행하세요.\n",
    "\n",
    "[예시: S3 버킷 정책]\n",
    "{\n",
    "  \"Effect\": \"Allow\",\n",
    "  \"Principal\": {\n",
    "    \"Service\": [\n",
    "      \"athena.amazonaws.com\",\n",
    "      \"glue.amazonaws.com\"\n",
    "    ],\n",
    "    \"AWS\": \"arn:aws:iam::<YOUR_ACCOUNT_ID>:role/<YOUR_ATHENA_GLUE_ROLE>\"\n",
    "  },\n",
    "  \"Action\": [\n",
    "    \"s3:GetObject\",\n",
    "    \"s3:PutObject\",\n",
    "    \"s3:DeleteObject\",\n",
    "    \"s3:ListBucket\"\n",
    "  ],\n",
    "  \"Resource\": [\n",
    "    \"arn:aws:s3:::personal-golight-image-bucket/dataset/*\",\n",
    "    \"arn:aws:s3:::personal-golight-image-bucket/dataset\"\n",
    "  ]\n",
    "}\n",
    "\n",
    "[참고]\n",
    "- 권한 변경 후에도 문제가 지속되면, Lake Formation에서 '데이터 위치 권한'과 '테이블 권한'을 모두 확인하세요.\n",
    "- 자세한 내용은 AWS 공식 문서(https://docs.aws.amazon.com/ko_kr/athena/latest/ug/lake-formation.html) 참고\n",
    "\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
