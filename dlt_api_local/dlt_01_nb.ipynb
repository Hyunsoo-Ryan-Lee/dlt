{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt, requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://jaffle-shop.scalevector.ai/api/v1/customers\"\n",
    "\n",
    "req = requests.get(URL)\n",
    "\n",
    "df = pd.json_normalize(req.json())\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. dlt를 사용하여 S3의 CSV 파일을 데이터 소스로 읽어오는 코드 예시입니다.\n",
    "\n",
    "import dlt\n",
    "import pandas as pd\n",
    "from dlt.sources.filesystem import filesystem\n",
    "\n",
    "# dlt의 s3 파일 커넥터를 사용하여 S3의 CSV 파일을 읽어옵니다.\n",
    "# dlt.sources.filesystem.filesystem_source를 사용합니다.\n",
    "\n",
    "filesystem_source = filesystem(\n",
    "  bucket_url=\"s3://lakeformation-demo-hyunsoo/dlt_dataset/\",\n",
    "  file_glob=\"*.csv\"\n",
    ")\n",
    "\n",
    "# 데이터를 DataFrame으로 변환\n",
    "emp_df = pd.DataFrame(list(filesystem_source))\n",
    "\n",
    "# 데이터 확인\n",
    "emp_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3에서 읽은 emp_df(DataFrame)를 PostgreSQL에 저장하는 dlt 파이프라인 생성\n",
    "emp_pg_pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"emp_data_pg_pipeline3\",\n",
    "    destination=\"postgres\",\n",
    "    dataset_name=\"emp_data_pg\"\n",
    ")\n",
    "\n",
    "# emp_df를 \"employees\" 테이블로 적재\n",
    "emp_pg_load_info = emp_pg_pipeline.run(\n",
    "    data=filesystem_source,\n",
    "    table_name=\"employees3\"\n",
    ")\n",
    "\n",
    "print(\"S3 CSV 데이터를 PostgreSQL에 저장한 결과:\", emp_pg_load_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"customer_data_pipeline\",\n",
    "    destination=\"filesystem\",\n",
    "    dataset_name=\"customer_data\"\n",
    ")\n",
    "\n",
    "load_info = pipeline.run(\n",
    "    data=df, \n",
    "    table_name=\"customers\"\n",
    "    )\n",
    "\n",
    "print(\"파이프라인 실행 결과:\", load_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet('./api_data_dir/customer_data/customers/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "\n",
    "# PostgreSQL로 데이터를 저장하는 dlt 파이프라인 생성\n",
    "# 동일 코드라도 데이터가 다르면 pipeline_name을 다르게 설정해야 함\n",
    "pg_pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"customer_data_pg_pipeline2\",\n",
    "    destination=\"postgres\",\n",
    "    dataset_name=\"customer_data_pg\"\n",
    ")\n",
    "\n",
    "# 데이터 적재 실행\n",
    "pg_load_info = pg_pipeline.run(\n",
    "    data=df,\n",
    "    table_name=\"stores\"\n",
    ")\n",
    "\n",
    "print(\"PostgreSQL 파이프라인 실행 결과:\", pg_load_info)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "\n",
    "# AWS Athena로 데이터를 저장하는 dlt 파이프라인 생성\n",
    "athena_pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"customer_data_athena_pipeline\",\n",
    "    destination=\"athena\",\n",
    "    dataset_name=\"customer_data_athena\"\n",
    ")\n",
    "\n",
    "# 데이터 적재 실행\n",
    "athena_load_info = athena_pipeline.run(\n",
    "    data=df,\n",
    "    table_name=\"customers\"\n",
    ")\n",
    "\n",
    "print(\"Athena 파이프라인 실행 결과:\", athena_load_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Athena에서 S3 버킷에 접근 권한이 없어서 발생하는 에러입니다.\n",
    "# Lake Formation 또는 S3 버킷의 권한 설정을 확인해야 합니다.\n",
    "# 아래는 문제 해결을 위한 안내 코드와 설명입니다.\n",
    "\n",
    "print(\"\"\"\n",
    "[문제 원인]\n",
    "- Athena가 S3 버킷(s3://personal-golight-image-bucket/dataset/customer_data_athena/_dlt_pipeline_state)에 접근할 권한이 없습니다.\n",
    "- Lake Formation 또는 S3 버킷의 권한 정책에서 Athena(및 Glue, Data Catalog) 역할/사용자에게 충분한 권한이 부여되어야 합니다.\n",
    "\n",
    "[해결 방법]\n",
    "1. AWS 콘솔에서 S3 버킷(s3://personal-golight-image-bucket)에 대해 다음 권한을 부여하세요.\n",
    "   - s3:GetObject, s3:PutObject, s3:DeleteObject, s3:ListBucket 등\n",
    "   - 권한을 부여할 주체: Athena, Glue, Data Catalog에서 사용하는 IAM Role 또는 User\n",
    "\n",
    "2. Lake Formation을 사용하는 경우:\n",
    "   - Lake Formation 콘솔에서 해당 S3 경로에 대해 Athena, Glue, Data Catalog에 '데이터 위치 권한'을 부여하세요.\n",
    "   - '데이터 위치' 등록 및 '데이터 위치 권한' 부여 필요\n",
    "\n",
    "3. 권한이 정상적으로 부여된 후 파이프라인을 다시 실행하세요.\n",
    "\n",
    "[예시: S3 버킷 정책]\n",
    "{\n",
    "  \"Effect\": \"Allow\",\n",
    "  \"Principal\": {\n",
    "    \"Service\": [\n",
    "      \"athena.amazonaws.com\",\n",
    "      \"glue.amazonaws.com\"\n",
    "    ],\n",
    "    \"AWS\": \"arn:aws:iam::<YOUR_ACCOUNT_ID>:role/<YOUR_ATHENA_GLUE_ROLE>\"\n",
    "  },\n",
    "  \"Action\": [\n",
    "    \"s3:GetObject\",\n",
    "    \"s3:PutObject\",\n",
    "    \"s3:DeleteObject\",\n",
    "    \"s3:ListBucket\"\n",
    "  ],\n",
    "  \"Resource\": [\n",
    "    \"arn:aws:s3:::personal-golight-image-bucket/dataset/*\",\n",
    "    \"arn:aws:s3:::personal-golight-image-bucket/dataset\"\n",
    "  ]\n",
    "}\n",
    "\n",
    "[참고]\n",
    "- 권한 변경 후에도 문제가 지속되면, Lake Formation에서 '데이터 위치 권한'과 '테이블 권한'을 모두 확인하세요.\n",
    "- 자세한 내용은 AWS 공식 문서(https://docs.aws.amazon.com/ko_kr/athena/latest/ug/lake-formation.html) 참고\n",
    "\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt, requests\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_customer_data(url: str) -> pd.DataFrame:\n",
    "    \"\"\"API로부터 고객 데이터를 가져오는 함수\"\"\"\n",
    "    response = requests.get(url)\n",
    "    df = pd.json_normalize(response.json())\n",
    "    return df\n",
    "    \n",
    "def load_to_s3(data: pd.DataFrame, **kwargs):\n",
    "    \"\"\"데이터를 S3에 적재하는 함수\"\"\"\n",
    "    pipeline = dlt.pipeline(\n",
    "        pipeline_name=kwargs[\"pipeline_name\"],\n",
    "        destination=\"filesystem\", \n",
    "        dataset_name=kwargs[\"dataset_name\"]\n",
    "    )\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = \"https://jaffle-shop.scalevector.ai/api/v1/customers\"\n",
    "\n",
    "# 데이터 가져오기\n",
    "customer_df = get_customer_data(API_URL)\n",
    "\n",
    "# S3에 데이터 적재\n",
    "pipeline = load_to_s3(\n",
    "    data=customer_df,\n",
    "    pipeline_name=\"customer_data_pipeline\",\n",
    "    dataset_name=\"customer_data\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.run(\n",
    "    data=customer_df, \n",
    "    table_name=\"customers\",\n",
    "    write_disposition=\"append\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt, requests\n",
    "import pandas as pd\n",
    "from dlt.sources.filesystem import filesystem\n",
    "\n",
    "\n",
    "def get_customer_data(url: str) -> pd.DataFrame:\n",
    "    \"\"\"API로부터 고객 데이터를 가져오는 함수\"\"\"\n",
    "    response = requests.get(url)\n",
    "    df = pd.json_normalize(response.json())\n",
    "    return df\n",
    "\n",
    "def load_to_s3(data: pd.DataFrame, **kwargs):\n",
    "    \"\"\"데이터를 S3에 적재하는 함수\"\"\"\n",
    "    pipeline = dlt.pipeline(\n",
    "        pipeline_name=kwargs[\"pipeline_name\"],\n",
    "        destination=\"filesystem\", \n",
    "        dataset_name=kwargs[\"dataset_name\"]\n",
    "    )\n",
    "    return pipeline\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # API URL 설정\n",
    "    API_URL = \"https://jaffle-shop.scalevector.ai/api/v1/customers\"\n",
    "    \n",
    "    # 데이터 가져오기\n",
    "    customer_df = get_customer_data(API_URL)\n",
    "    \n",
    "    # S3에 데이터 적재\n",
    "    pipeline = load_to_s3(\n",
    "        data=customer_df,\n",
    "        pipeline_name=\"customer_data_pipeline\",\n",
    "        dataset_name=\"customer_data\"\n",
    "        )\n",
    "    \n",
    "    pipeline.run(data=customer_df, table_name=\"customers\")\n",
    "    \n",
    "    print(\"파이프라인 실행 결과:\", pipeline)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">d:\\코드잇\\codeit_DA_lecture\\lecture\\Lib\\site-packages\\airflow\\__init__.py:</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">45</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> RuntimeWarning</span><span style=\"color: #808000; text-decoration-color: #808000\">: Airflow currently can be run on POSIX-compliant Operating Systems. For development, it is regularly tested on fairly modern Linux Distros and recent versions of macOS. On Windows you can run it via WSL2 </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">Windows Subsystem for Linux </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">2</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\"> or via Linux Containers. The work to add Windows support is tracked via </span><span style=\"color: #808000; text-decoration-color: #808000; text-decoration: underline\">https://github.com/apache/airflow/issues/10388,</span><span style=\"color: #808000; text-decoration-color: #808000\"> but it is not a high priority.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33md:\\코드잇\\codeit_DA_lecture\\lecture\\Lib\\site-packages\\airflow\\__init__.py:\u001b[0m\u001b[1;33m45\u001b[0m\u001b[1;33m RuntimeWarning\u001b[0m\u001b[33m: Airflow currently can be run on POSIX-compliant Operating Systems. For development, it is regularly tested on fairly modern Linux Distros and recent versions of macOS. On Windows you can run it via WSL2 \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mWindows Subsystem for Linux \u001b[0m\u001b[1;33m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m or via Linux Containers. The work to add Windows support is tracked via \u001b[0m\u001b[4;33mhttps://github.com/apache/airflow/issues/10388,\u001b[0m\u001b[33m but it is not a high priority.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">d:\\코드잇\\codeit_DA_lecture\\lecture\\Lib\\site-packages\\airflow\\__init__.py:</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">45</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> RuntimeWarning</span><span style=\"color: #808000; text-decoration-color: #808000\">: Airflow currently can be run on POSIX-compliant Operating Systems. For development, it is regularly tested on fairly modern Linux Distros and recent versions of macOS. On Windows you can run it via WSL2 </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">Windows Subsystem for Linux </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">2</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">)</span><span style=\"color: #808000; text-decoration-color: #808000\"> or via Linux Containers. The work to add Windows support is tracked via </span><span style=\"color: #808000; text-decoration-color: #808000; text-decoration: underline\">https://github.com/apache/airflow/issues/10388,</span><span style=\"color: #808000; text-decoration-color: #808000\"> but it is not a high priority.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33md:\\코드잇\\codeit_DA_lecture\\lecture\\Lib\\site-packages\\airflow\\__init__.py:\u001b[0m\u001b[1;33m45\u001b[0m\u001b[1;33m RuntimeWarning\u001b[0m\u001b[33m: Airflow currently can be run on POSIX-compliant Operating Systems. For development, it is regularly tested on fairly modern Linux Distros and recent versions of macOS. On Windows you can run it via WSL2 \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mWindows Subsystem for Linux \u001b[0m\u001b[1;33m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m or via Linux Containers. The work to add Windows support is tracked via \u001b[0m\u001b[4;33mhttps://github.com/apache/airflow/issues/10388,\u001b[0m\u001b[33m but it is not a high priority.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigQuery 파이프라인 실행 결과: Pipeline api_data_pipeline_bq load step completed in 13.44 seconds\n",
      "1 load package(s) were loaded to destination bigquery and into dataset bq_dlt\n",
      "The bigquery destination used de-study-sa@codeit-hyunsoo.iam.gserviceaccount.com@codeit-hyunsoo location to store data\n",
      "Load package 1752074983.7651272 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "import dlt, requests\n",
    "import pandas as pd\n",
    "\n",
    "def get_customer_data(url: str) -> pd.DataFrame:\n",
    "    \"\"\"API로부터 고객 데이터를 가져오는 함수\"\"\"\n",
    "    response = requests.get(url)\n",
    "    df = pd.json_normalize(response.json())\n",
    "    return df\n",
    "\n",
    "def load_to_bigquery(data: pd.DataFrame, **kwargs):\n",
    "    \"\"\"데이터를 BigQuery에 적재하는 함수\"\"\"\n",
    "    pipeline = dlt.pipeline(\n",
    "        pipeline_name=kwargs[\"pipeline_name\"],\n",
    "        destination=\"bigquery\", \n",
    "        dataset_name=kwargs[\"dataset_name\"]\n",
    "    )\n",
    "    return pipeline\n",
    "\n",
    "# API URL 설정\n",
    "API_URL = \"https://jaffle-shop.scalevector.ai/api/v1/customers\"\n",
    "\n",
    "# 데이터 가져오기\n",
    "customer_df = get_customer_data(API_URL)\n",
    "\n",
    "# BigQuery에 데이터 적재\n",
    "pipeline = load_to_bigquery(\n",
    "    data=customer_df,\n",
    "    pipeline_name=\"api_data_pipeline_bq\",\n",
    "    dataset_name=\"bq_dlt\"\n",
    ")\n",
    "\n",
    "result = pipeline.run(data=customer_df, table_name=\"customers\")\n",
    "\n",
    "print(\"BigQuery 파이프라인 실행 결과:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
