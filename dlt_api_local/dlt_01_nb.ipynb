{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt, requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://jaffle-shop.scalevector.ai/api/v1/customers\"\n",
    "\n",
    "req = requests.get(URL)\n",
    "\n",
    "df = pd.json_normalize(req.json())\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. dlt를 사용하여 S3의 CSV 파일을 데이터 소스로 읽어오는 코드 예시입니다.\n",
    "\n",
    "import dlt\n",
    "import pandas as pd\n",
    "from dlt.sources.filesystem import filesystem\n",
    "\n",
    "# dlt의 s3 파일 커넥터를 사용하여 S3의 CSV 파일을 읽어옵니다.\n",
    "# dlt.sources.filesystem.filesystem_source를 사용합니다.\n",
    "\n",
    "filesystem_source = filesystem(\n",
    "  bucket_url=\"s3://lakeformation-demo-hyunsoo/dlt_dataset/\",\n",
    "  file_glob=\"*.csv\"\n",
    ")\n",
    "\n",
    "# 데이터를 DataFrame으로 변환\n",
    "emp_df = pd.DataFrame(list(filesystem_source))\n",
    "\n",
    "# 데이터 확인\n",
    "emp_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3에서 읽은 emp_df(DataFrame)를 PostgreSQL에 저장하는 dlt 파이프라인 생성\n",
    "emp_pg_pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"emp_data_pg_pipeline3\",\n",
    "    destination=\"postgres\",\n",
    "    dataset_name=\"emp_data_pg\"\n",
    ")\n",
    "\n",
    "# emp_df를 \"employees\" 테이블로 적재\n",
    "emp_pg_load_info = emp_pg_pipeline.run(\n",
    "    data=filesystem_source,\n",
    "    table_name=\"employees3\"\n",
    ")\n",
    "\n",
    "print(\"S3 CSV 데이터를 PostgreSQL에 저장한 결과:\", emp_pg_load_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"customer_data_pipeline\",\n",
    "    destination=\"filesystem\",\n",
    "    dataset_name=\"customer_data\"\n",
    ")\n",
    "\n",
    "load_info = pipeline.run(\n",
    "    data=df, \n",
    "    table_name=\"customers\"\n",
    "    )\n",
    "\n",
    "print(\"파이프라인 실행 결과:\", load_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet('./api_data_dir/customer_data/customers/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "\n",
    "# PostgreSQL로 데이터를 저장하는 dlt 파이프라인 생성\n",
    "# 동일 코드라도 데이터가 다르면 pipeline_name을 다르게 설정해야 함\n",
    "pg_pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"customer_data_pg_pipeline2\",\n",
    "    destination=\"postgres\",\n",
    "    dataset_name=\"customer_data_pg\"\n",
    ")\n",
    "\n",
    "# 데이터 적재 실행\n",
    "pg_load_info = pg_pipeline.run(\n",
    "    data=df,\n",
    "    table_name=\"stores\"\n",
    ")\n",
    "\n",
    "print(\"PostgreSQL 파이프라인 실행 결과:\", pg_load_info)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "\n",
    "# AWS Athena로 데이터를 저장하는 dlt 파이프라인 생성\n",
    "athena_pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"customer_data_athena_pipeline\",\n",
    "    destination=\"athena\",\n",
    "    dataset_name=\"customer_data_athena\"\n",
    ")\n",
    "\n",
    "# 데이터 적재 실행\n",
    "athena_load_info = athena_pipeline.run(\n",
    "    data=df,\n",
    "    table_name=\"customers\"\n",
    ")\n",
    "\n",
    "print(\"Athena 파이프라인 실행 결과:\", athena_load_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Athena에서 S3 버킷에 접근 권한이 없어서 발생하는 에러입니다.\n",
    "# Lake Formation 또는 S3 버킷의 권한 설정을 확인해야 합니다.\n",
    "# 아래는 문제 해결을 위한 안내 코드와 설명입니다.\n",
    "\n",
    "print(\"\"\"\n",
    "[문제 원인]\n",
    "- Athena가 S3 버킷(s3://personal-golight-image-bucket/dataset/customer_data_athena/_dlt_pipeline_state)에 접근할 권한이 없습니다.\n",
    "- Lake Formation 또는 S3 버킷의 권한 정책에서 Athena(및 Glue, Data Catalog) 역할/사용자에게 충분한 권한이 부여되어야 합니다.\n",
    "\n",
    "[해결 방법]\n",
    "1. AWS 콘솔에서 S3 버킷(s3://personal-golight-image-bucket)에 대해 다음 권한을 부여하세요.\n",
    "   - s3:GetObject, s3:PutObject, s3:DeleteObject, s3:ListBucket 등\n",
    "   - 권한을 부여할 주체: Athena, Glue, Data Catalog에서 사용하는 IAM Role 또는 User\n",
    "\n",
    "2. Lake Formation을 사용하는 경우:\n",
    "   - Lake Formation 콘솔에서 해당 S3 경로에 대해 Athena, Glue, Data Catalog에 '데이터 위치 권한'을 부여하세요.\n",
    "   - '데이터 위치' 등록 및 '데이터 위치 권한' 부여 필요\n",
    "\n",
    "3. 권한이 정상적으로 부여된 후 파이프라인을 다시 실행하세요.\n",
    "\n",
    "[예시: S3 버킷 정책]\n",
    "{\n",
    "  \"Effect\": \"Allow\",\n",
    "  \"Principal\": {\n",
    "    \"Service\": [\n",
    "      \"athena.amazonaws.com\",\n",
    "      \"glue.amazonaws.com\"\n",
    "    ],\n",
    "    \"AWS\": \"arn:aws:iam::<YOUR_ACCOUNT_ID>:role/<YOUR_ATHENA_GLUE_ROLE>\"\n",
    "  },\n",
    "  \"Action\": [\n",
    "    \"s3:GetObject\",\n",
    "    \"s3:PutObject\",\n",
    "    \"s3:DeleteObject\",\n",
    "    \"s3:ListBucket\"\n",
    "  ],\n",
    "  \"Resource\": [\n",
    "    \"arn:aws:s3:::personal-golight-image-bucket/dataset/*\",\n",
    "    \"arn:aws:s3:::personal-golight-image-bucket/dataset\"\n",
    "  ]\n",
    "}\n",
    "\n",
    "[참고]\n",
    "- 권한 변경 후에도 문제가 지속되면, Lake Formation에서 '데이터 위치 권한'과 '테이블 권한'을 모두 확인하세요.\n",
    "- 자세한 내용은 AWS 공식 문서(https://docs.aws.amazon.com/ko_kr/athena/latest/ug/lake-formation.html) 참고\n",
    "\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt, requests\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_customer_data(url: str) -> pd.DataFrame:\n",
    "    \"\"\"API로부터 고객 데이터를 가져오는 함수\"\"\"\n",
    "    response = requests.get(url)\n",
    "    df = pd.json_normalize(response.json())\n",
    "    return df\n",
    "    \n",
    "def load_to_s3(data: pd.DataFrame, **kwargs):\n",
    "    \"\"\"데이터를 S3에 적재하는 함수\"\"\"\n",
    "    pipeline = dlt.pipeline(\n",
    "        pipeline_name=kwargs[\"pipeline_name\"],\n",
    "        destination=\"filesystem\", \n",
    "        dataset_name=kwargs[\"dataset_name\"]\n",
    "    )\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = \"https://jaffle-shop.scalevector.ai/api/v1/customers\"\n",
    "\n",
    "# 데이터 가져오기\n",
    "customer_df = get_customer_data(API_URL)\n",
    "\n",
    "# S3에 데이터 적재\n",
    "pipeline = load_to_s3(\n",
    "    data=customer_df,\n",
    "    pipeline_name=\"customer_data_pipeline\",\n",
    "    dataset_name=\"customer_data\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.run(\n",
    "    data=customer_df, \n",
    "    table_name=\"customers\",\n",
    "    write_disposition=\"append\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt, requests\n",
    "import pandas as pd\n",
    "from dlt.sources.filesystem import filesystem\n",
    "\n",
    "\n",
    "def get_customer_data(url: str) -> pd.DataFrame:\n",
    "    \"\"\"API로부터 고객 데이터를 가져오는 함수\"\"\"\n",
    "    response = requests.get(url)\n",
    "    df = pd.json_normalize(response.json())\n",
    "    return df\n",
    "\n",
    "def load_to_s3(data: pd.DataFrame, **kwargs):\n",
    "    \"\"\"데이터를 S3에 적재하는 함수\"\"\"\n",
    "    pipeline = dlt.pipeline(\n",
    "        pipeline_name=kwargs[\"pipeline_name\"],\n",
    "        destination=\"filesystem\", \n",
    "        dataset_name=kwargs[\"dataset_name\"]\n",
    "    )\n",
    "    return pipeline\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # API URL 설정\n",
    "    API_URL = \"https://jaffle-shop.scalevector.ai/api/v1/customers\"\n",
    "    \n",
    "    # 데이터 가져오기\n",
    "    customer_df = get_customer_data(API_URL)\n",
    "    \n",
    "    # S3에 데이터 적재\n",
    "    pipeline = load_to_s3(\n",
    "        data=customer_df,\n",
    "        pipeline_name=\"customer_data_pipeline\",\n",
    "        dataset_name=\"customer_data\"\n",
    "        )\n",
    "    \n",
    "    pipeline.run(data=customer_df, table_name=\"customers\")\n",
    "    \n",
    "    print(\"파이프라인 실행 결과:\", pipeline)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCS의 parquet 파일을 소스로 하여 PostgreSQL에 적재하는 dlt 파이프라인 예시입니다.\n",
    "\n",
    "import dlt\n",
    "import pandas as pd\n",
    "from dlt.sources.filesystem import filesystem, read_parquet\n",
    "\n",
    "source = filesystem(\n",
    "    bucket_url=\"gs://hyunsoo_de_bucket/dataset/pokemon/\",\n",
    "    file_glob=\"*.parquet\"\n",
    ")\n",
    "\n",
    "filesystem_pipe = (source | read_parquet())\n",
    "\n",
    "filesystem_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dlt 파이프라인 생성 (destination: postgres)\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"gcs_to_postgres_pipeline\",\n",
    "    destination=\"duckdb\",\n",
    ")\n",
    "    # dataset_name=\"public\"  # PostgreSQL의 스키마명\n",
    "\n",
    "# 파이프라인 실행 (parquet 파일을 customers 테이블로 적재, 필요시 테이블명 변경)\n",
    "load_info = pipeline.run(\n",
    "    filesystem_pipe.with_name(\"pokemon\"),\n",
    ")\n",
    "    # table_name=\"pokemon\",\n",
    "    # write_disposition=\"replace\"  # 기존 테이블 덮어쓰기, append로 변경 가능\n",
    "\n",
    "print(\"파이프라인 실행 결과:\", load_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GCS 를 source로 하는 파이프라인 생성\n",
    "import dlt\n",
    "from dlt.sources.filesystem import filesystem\n",
    "from dlt.sources.filesystem import readers\n",
    "\n",
    "@dlt.source\n",
    "def gcs_pokemon_source():\n",
    "    @dlt.resource(table_name=\"emp\", write_disposition=\"replace\")\n",
    "    def get_parquet_data():\n",
    "        yield from readers(\n",
    "            bucket_url=\"gs://hyunsoo_de_bucket/dataset/\",\n",
    "            file_glob=\"emp.parquet\"\n",
    "        ).read_parquet()\n",
    "\n",
    "    @dlt.resource(table_name=\"emp_csv\", write_disposition=\"replace\")\n",
    "    def get_csv_data():\n",
    "        yield from readers(\n",
    "            bucket_url=\"gs://hyunsoo_de_bucket/dataset/\",\n",
    "            file_glob=\"emp.csv\"\n",
    "        ).read_csv()\n",
    "\n",
    "    return get_csv_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pipeline = dlt.pipeline(\n",
    "        pipeline_name=\"gcs_pokemon_pipeline\",\n",
    "        destination=\"postgres\",\n",
    "        dataset_name=\"docker_pg\"\n",
    "    )\n",
    "    result = pipeline.run(gcs_pokemon_source())\n",
    "    print(\"파이프라인 실행 결과:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Database를 source로 하는 파이프라인 생성\n",
    "# https://dlthub.com/docs/dlt-ecosystem/verified-sources/sql_database/configuration\n",
    "\n",
    "import dlt\n",
    "from dlt.sources.filesystem import filesystem\n",
    "from dlt.sources.filesystem import readers\n",
    "\n",
    "@dlt.source\n",
    "def gcs_pokemon_source():\n",
    "    @dlt.resource(table_name=\"emp_c\", write_disposition=\"replace\")\n",
    "    def get_csv_data():\n",
    "        yield from readers(\n",
    "            bucket_url=\"gs://hyunsoo_de_bucket/dataset/\",\n",
    "            file_glob=\"emp.csv\"\n",
    "        ).read_csv()\n",
    "\n",
    "    return get_csv_data\n",
    "\n",
    "aa = gcs_pokemon_source()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa.run(\n",
    "    destination=\"postgres\",\n",
    "    dataset_name=\"docker_pg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PG 데이터베이스를 소스로 하는 파이프라인 생성\n",
    "# uv add 'dlt[sql_database]'\n",
    "import dlt\n",
    "from dlt.sources.sql_database import sql_database\n",
    "\n",
    "# 테이블 이름을 지정하면 해당 테이블만 불러옴\n",
    "# 테이블 이름을 지정하지 않으면 데이터가 있는 모든 테이블을 일단 불러옴\n",
    "source = sql_database(\n",
    "    table_names=['ducklake_metadata', 'ducklake_schema']\n",
    ")\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"pg_to_gcs\",\n",
    "    destination='filesystem',\n",
    "    dataset_name=\"pg_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<@dlt.source(name='sql_database', n_resources=21, resources=['ducklake_metadata', 'ducklake_snapshot', 'ducklake_snapshot_changes', 'ducklake_schema', 'ducklake_table', 'ducklake_view', 'ducklake_tag', 'ducklake_column_tag', 'ducklake_data_file', 'ducklake_file_column_statistics', 'ducklake_delete_file', 'ducklake_column', 'ducklake_table_stats', 'ducklake_table_column_stats', 'ducklake_partition_info', 'ducklake_partition_column', 'ducklake_file_partition_value', 'ducklake_files_scheduled_for_deletion', 'ducklake_inlined_data_tables', 'ducklake_column_mapping', 'ducklake_name_mapping'])>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 22:24:13,584|[WARNING]|44763|8685697728|dlt|validate.py|verify_normalized_table:57|In schema `sql_database`: The following columns in table 'ducklake_snapshot' did not receive any data during this load and therefore could not have their types inferred:\n",
      "  - file_order\n",
      "  - partition_id\n",
      "  - encryption_key\n",
      "  - partial_file_info\n",
      "  - mapping_id\n",
      "  - parent_column\n",
      "\n",
      "Unless type hints are provided, these columns will not be materialized in the destination.\n",
      "One way to provide type hints is to use the 'columns' argument in the '@dlt.resource' decorator.  For example:\n",
      "\n",
      "@dlt.resource(columns={'file_order': {'data_type': 'text'}})\n",
      "\n",
      "2025-07-10 22:24:13,585|[WARNING]|44763|8685697728|dlt|validate.py|verify_normalized_table:57|In schema `sql_database`: The following columns in table 'ducklake_snapshot_changes' did not receive any data during this load and therefore could not have their types inferred:\n",
      "  - file_order\n",
      "  - partition_id\n",
      "  - encryption_key\n",
      "  - partial_file_info\n",
      "  - mapping_id\n",
      "  - parent_column\n",
      "\n",
      "Unless type hints are provided, these columns will not be materialized in the destination.\n",
      "One way to provide type hints is to use the 'columns' argument in the '@dlt.resource' decorator.  For example:\n",
      "\n",
      "@dlt.resource(columns={'file_order': {'data_type': 'text'}})\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline pg_to_gcs load step completed in 3.46 seconds\n",
      "1 load package(s) were loaded to destination filesystem and into dataset pg_data\n",
      "The filesystem destination used gs://hyunsoo_de_bucket/dlt_destination location to store data\n",
      "Load package 1752153853.246578 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "# Run the pipeline\n",
    "\"\"\"\n",
    "pipeline.run(data=source) 이렇게만 적으면 데이터가 있는 모든 테이블을 로드함.\n",
    "ROW가 0이면 로드하지 않음.\n",
    "\"\"\"\n",
    "info = pipeline.run(\n",
    "    data=source, \n",
    "    )\n",
    "    # table_name=\"ducklake_snapshot_changes\",\n",
    "    # write_disposition=\"replace\"\n",
    "\n",
    "# Print load info\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
